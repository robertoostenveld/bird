Title: the principle of inverse effectiveness in audiovisual speech perception
Description: "We assessed how synchronous speech listening and lipreading affects\
  \ speech recognition in acoustic noise. In simple audiovisual\r\nperceptual tasks,\
  \ inverse effectiveness is often observed, which holds that the weaker the unimodal\
  \ stimuli, or the poorer their\r\nsignal-to-noise ratio, the stronger the audiovisual\
  \ benefit. So far, however, inverse effectiveness has not been demonstrated for\r\
  \ncomplex audiovisual speech stimuli. Here we assess whether this multisensory integration\
  \ effect can also be observed for the\r\nrecognizability of spoken words.\r\nTo\
  \ that end, we presented audiovisual sentences to 18 native-Dutch normal-hearing\
  \ participants, who had to identify the spoken\r\nwords from a finite list. Speech-recognition\
  \ performance was determined for auditory-only, visual-only (lipreading) and auditoryvisual\r\
  \nconditions. To modulate acoustic task difficulty, we systematically varied the\
  \ auditory signal-to-noise ratio. In line with a\r\ncommonly-observed multisensory\
  \ enhancement on speech recognition, audiovisual words were more easily recognized\
  \ than\r\nauditory-only words (recognition thresholds of -15 dB and -12 dB, respectively).\r\
  \nWe here show that the difficulty of recognizing a particular word, either acoustically\
  \ or visually, determines the occurrence of\r\ninverse effectiveness in audiovisual\
  \ word integration. Thus, words that are better heard or recognized through lipreading,\r\
  \nbenefit less from bimodal presentation.\r\nAudiovisual performance at the lowest\
  \ acoustic signal-to-noise ratios (45%) fell below the visual recognition rates\
  \ (60%), reflecting\r\nan actual deterioration of lipreading in the presence of\
  \ excessive acoustic noise. This suggests that the brain may adopt a\r\nstrategy\
  \ in which attention has to be divided between listening and lipreading."
CollectionIdentifier: di.dcn.DSC_626810_0003_650
PersistentIdentifier: 10.34973/egzg-gh08
PersistentURL: https://doi.org/10.34973/egzg-gh08
ProjectIdentifier: '626810_0003'
OrganisationalUnit: DCN
Type: Data Sharing Collection
PreservationTime: '10'
State: Published
Publisher: Radboud University
DataUseAgreement: CC-BY-4.0
EthicalApprovals: ''
FreeKeywords:
- [speech recognition in noise]
- [' multisensory integration']
- [' listening']
- [' lipreading']
- [' audiovisual speech']
SFNKeywords:
- [D.17.l]
- [' D.03']
- [' D.02.h']
MeSHKeywords:
- [L01.143.506.423.676]
- [' L01.143.506.423.348']
- [' F02.463.593.071']
- [' F02.463.593']
Version: '1'
AssociatedPublications: 'DOI: https://doi.org/10.1101/585182; DOI: 10.3389/fnhum.2019.00335'
AssociatedData: ''
AssociatedAnalysisTools: ''
AssociatedPreregistrations: ''
Authors:
- [Luuk van de Rijt]
- [' Anja Roye']
- [' Emmanuel Mylanus']
- [' Opstal, A.J. van']
- [' Wanrooij, M.M. van']
Managers: ['Wanrooij, M.M. van']
Contributors: ['Opstal, A.J. van']
NumberOfFiles: '5'
Size: '662633'
Quota: '26843545600'
CreationDate: 18-Sep-2019 08:27:15
LastUpdateDate: 08-Oct-2021 12:11:43
EmbargoUntil: 18-Sep-2019 08:27:15
PublicationDate: 18-Sep-2019 08:41:06
ArchivalDate: ''
ViewersByDUA: '0'
ViewersByManager: '0'
Views: '289'
Downloads: '114'
DataVolume: '13172736'
Viewers: '237'
Downloaders: '63'
